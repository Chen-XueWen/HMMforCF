{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Gibbs Sampling for HMM_Pytorch_gpu.ipynb","provenance":[],"collapsed_sections":[],"machine_shape":"hm","mount_file_id":"19xzxM9J5Jt7r-s7XB-8LCBZ0YrTHoMUg","authorship_tag":"ABX9TyP194LZJgCw1dzjWN7NFZck"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"xoM7Nun63R6s"},"source":["# Packages"]},{"cell_type":"code","execution_count":1,"metadata":{"_cell_guid":"b1f10f39-91d4-45f1-b62f-d1f590e50438","_kg_hide-input":false,"_kg_hide-output":false,"_uuid":"7edd510ba8ac857514e34d6b38c0466d125cffb9","executionInfo":{"elapsed":6501,"status":"ok","timestamp":1649654933955,"user":{"displayName":"Chung I Lu","userId":"13493253019536068280"},"user_tz":-480},"id":"ICksXUZs-3KQ","trusted":true},"outputs":[],"source":["import numpy as np\n","import pandas as pd\n","import pickle\n","import torch\n","from torch.distributions import Dirichlet, Multinomial, Categorical, Beta\n","from datetime import datetime\n","from sklearn.preprocessing import MultiLabelBinarizer\n","\n","try:\n","    import google.colab\n","    IN_COLAB = True\n","    path = '/content/drive/MyDrive/PhD/Modules/CS5340 Uncertainty Modeling in AI/Project/'\n","except:    \n","    IN_COLAB = False\n","    path = './'\n","\n","device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n","\n","n_splits = int(2) # excludes final chunk\n","splits = int(400)"]},{"cell_type":"markdown","metadata":{"id":"gldC8sScQBTB"},"source":["# Dataset"]},{"cell_type":"code","execution_count":2,"metadata":{"executionInfo":{"elapsed":15732,"status":"ok","timestamp":1649654949681,"user":{"displayName":"Chung I Lu","userId":"13493253019536068280"},"user_tz":-480},"id":"mUmFmssxKl22"},"outputs":[],"source":["users_ds = np.load(path + 'users_ds_dense.npy', allow_pickle=True)"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":15,"status":"ok","timestamp":1649654949682,"user":{"displayName":"Chung I Lu","userId":"13493253019536068280"},"user_tz":-480},"id":"K1QbMOKwKvUM","outputId":"919514f7-8e8f-4629-d022-bc1294127cc8"},"outputs":[{"output_type":"stream","name":"stdout","text":["Shape of dataset: (1212, 73, 17768)\n","Number of users: 1212\n","Number of movie titles: 17768\n","Number of periods: 73\n"]}],"source":["# size of training dataset\n","U, T, N = users_ds.shape\n","print('Shape of dataset:', users_ds.shape)\n","print('Number of users:', U)\n","print('Number of movie titles:', N)\n","print('Number of periods:', T)"]},{"cell_type":"code","execution_count":4,"metadata":{"executionInfo":{"elapsed":6,"status":"ok","timestamp":1649654949683,"user":{"displayName":"Chung I Lu","userId":"13493253019536068280"},"user_tz":-480},"id":"AOn8l1KrLUvk"},"outputs":[],"source":["t_predict = -1                                          # index for holdout period for prediction\n","test_ds = users_ds[:,t_predict,:]                       # extract holdout period from dataset\n","test_ds = test_ds[:,np.newaxis,:]                       # align # of dimensions\n","users_ds = users_ds[:,:t_predict,:]                     # specify training periods"]},{"cell_type":"code","execution_count":5,"metadata":{"executionInfo":{"elapsed":1761,"status":"ok","timestamp":1649654951438,"user":{"displayName":"Chung I Lu","userId":"13493253019536068280"},"user_tz":-480},"id":"wUj2_ruSK4Op","colab":{"base_uri":"https://localhost:8080/"},"outputId":"c938d469-a5de-48ba-d8aa-b687e0dff52b"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["(1212, 72)"]},"metadata":{},"execution_count":5}],"source":["# calculate and store total number of ratings per user in a period\n","users_Nt = np.sum(users_ds, axis=-1)    # number of movies rated by user in each time period\n","T = np.shape(users_ds)[1]               # final length of time period after trimming\n","users_Nt.shape "]},{"cell_type":"code","source":["users_ds = torch.tensor(users_ds, dtype=torch.float).to(device)\n","users_Nt = torch.tensor(users_Nt).to(device)"],"metadata":{"id":"fUnJ6RwfJsiP","executionInfo":{"status":"ok","timestamp":1649654965268,"user_tz":-480,"elapsed":13835,"user":{"displayName":"Chung I Lu","userId":"13493253019536068280"}}},"execution_count":6,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"J0NziYIX4hKG"},"source":["# Helper Functions"]},{"cell_type":"code","execution_count":7,"metadata":{"id":"s5vA1jv3KiYk","executionInfo":{"status":"ok","timestamp":1649654965268,"user_tz":-480,"elapsed":15,"user":{"displayName":"Chung I Lu","userId":"13493253019536068280"}}},"outputs":[],"source":["def logdotexp(A, B):\n","    max_A = A.max()\n","    max_B = B.max()\n","    C = torch.matmul(torch.exp(A - max_A), torch.exp(B - max_B))\n","    torch.log(C, out=C)\n","    C += max_A + max_B\n","    return C"]},{"cell_type":"code","execution_count":8,"metadata":{"executionInfo":{"elapsed":15,"status":"ok","timestamp":1649654965268,"user":{"displayName":"Chung I Lu","userId":"13493253019536068280"},"user_tz":-480},"id":"DMq-W5vm4zg1"},"outputs":[],"source":["def initialise_params_and_probs(pi_alpha, A_alpha, theta_alpha):\n","    K = len(pi_alpha)\n","    pi = Dirichlet(pi_alpha).sample().squeeze().to(device)                        # Initial state distribution\n","    A = torch.zeros((K,K)).to(device)                                                         # Transitional probabilities\n","    theta = torch.zeros((K,N)).to(device)                                                     # Multinomial probabilities\n","    for k in range(K):\n","        A[k,:] = Dirichlet(A_alpha[k,:]).sample().squeeze()\n","        theta[k,:] = Dirichlet(theta_alpha[k,:]).sample().squeeze()\n","    pi = torch.log(pi)\n","    A = torch.log(A)\n","    theta = torch.log(theta)\n","\n","    # NBD parameters\n","    a = torch.ones((K)).to(device)\n","    p = torch.FloatTensor(K).uniform_(0.7, 0.9).to(device)\n","    b = p / (1-p)\n","    return pi, A, theta, a, b\n","\n","# K = 5\n","# pi_alpha = torch.ones((K))\n","# A_alpha = torch.ones((K,K))\n","# theta_alpha = torch.ones((K,N))\n","# pi, A, theta, a, b = initialise_params_and_probs(pi_alpha, A_alpha, theta_alpha)"]},{"cell_type":"code","execution_count":9,"metadata":{"executionInfo":{"elapsed":15,"status":"ok","timestamp":1649654965269,"user":{"displayName":"Chung I Lu","userId":"13493253019536068280"},"user_tz":-480},"id":"DXRIyzEvStjB"},"outputs":[],"source":["def initialise_latent_states(pi, A, U, T):\n","    # latent states of all users across the periods\n","    latent_states = torch.zeros((U,T), dtype=int).to(device)\n","    latent_states[:,0] = torch.multinomial(torch.exp(pi), U, replacement=True)    # initial latent state assignments\n","    sample_A = torch.tile(torch.exp(A), (U,1,1)).to(device)\n","    for t in range(T-1):\n","        previous_states = latent_states[:,t]\n","        pvals = sample_A[t, previous_states] \n","        latent_states[:,t+1] = torch.multinomial(pvals, 1, replacement=True).squeeze()\n","        \n","    return latent_states\n","\n","# latent_states = initialise_latent_states(pi, A, U, T)\n","# print(latent_states.shape, latent_states[0])"]},{"cell_type":"code","execution_count":10,"metadata":{"id":"Gvlq58JobkgD","executionInfo":{"status":"ok","timestamp":1649654965269,"user_tz":-480,"elapsed":15,"user":{"displayName":"Chung I Lu","userId":"13493253019536068280"}}},"outputs":[],"source":["def update_b(latent_states, users_Nt, K, a):\n","    alpha = torch.ones((K), dtype=torch.float).to(device)\n","    beta = torch.tensor(users_Nt.shape[0] * users_Nt.shape[1] + 1).to(torch.float).to(device)\n","\n","    b = torch.zeros((K)).to(device)\n","    for i in range(K):\n","        indices = latent_states == i\n","        alpha[i] += users_Nt[indices].sum()\n","        p = Beta(alpha[i], beta).sample()\n","        b[i] = p / (1-p)\n","\n","    return b\n","\n","# b = update_b(latent_states, users_Nt, K, 1)\n","# print(b)"]},{"cell_type":"code","execution_count":11,"metadata":{"id":"HIJkmnIuYb2L","executionInfo":{"status":"ok","timestamp":1649654965269,"user_tz":-480,"elapsed":14,"user":{"displayName":"Chung I Lu","userId":"13493253019536068280"}}},"outputs":[],"source":["def update_theta(latent_states, dataset, K):\n","    alpha = torch.ones((K,N), dtype=torch.float).to(device)\n","    U = latent_states.shape[0]\n","    for u in range(U):\n","        user_states = latent_states[u,:]\n","        counts = dataset[u,:,:]\n","        for t in range(T):\n","            alpha[user_states[t]] += counts[t]\n","\n","    # print(alpha.sum() - K*N, dataset.sum()) # checksum\n","\n","    # theta = rng.dirichlet(alpha=alpha, size=1) # dirichlet does not accept array like for alpha unlike multinomial\n","\n","    theta = torch.zeros((K,N), dtype=torch.float).to(device)\n","    for k in range(K):\n","        theta[k,:] = Dirichlet(alpha[k,:]).sample().squeeze()\n","\n","    return torch.log(theta)\n","\n","# theta = update_theta(latent_states, users_ds, K)\n","# np.exp(theta).sum(axis=-1)"]},{"cell_type":"code","execution_count":12,"metadata":{"id":"RTU3Y4H1SryC","executionInfo":{"status":"ok","timestamp":1649654965270,"user_tz":-480,"elapsed":15,"user":{"displayName":"Chung I Lu","userId":"13493253019536068280"}}},"outputs":[],"source":["def update_A(latent_states, K):\n","    alpha = torch.ones((K,K), dtype=torch.float)\n","    T = latent_states.shape[1]\n","\n","    for t in range(T - 1):\n","        current_states = latent_states[:,t]\n","        next_states = latent_states[:,t+1]\n","        for i in range(len(current_states)):\n","            alpha[current_states[i], next_states[i]] += 1\n","    \n","    # print(alpha.sum() - K*K, U*(T-1)) # checksum\n","\n","    A = torch.zeros((K,K), dtype=torch.float).to(device)\n","    for k in range(K):\n","        A[k,:] = Dirichlet(A_alpha[k,:]).sample().squeeze()\n","    return torch.log(A)\n","\n","# A = update_A(latent_states, K)\n","# np.exp(A)"]},{"cell_type":"code","execution_count":13,"metadata":{"id":"SX650QntwHYL","executionInfo":{"status":"ok","timestamp":1649654965270,"user_tz":-480,"elapsed":15,"user":{"displayName":"Chung I Lu","userId":"13493253019536068280"}}},"outputs":[],"source":["def update_pi(latent_states, K):\n","    alpha = torch.ones((K), dtype=torch.float).to(device)\n","\n","    for i in range(K):\n","        alpha[i] += (latent_states[:,0] == i).sum()\n","\n","    # print(alpha.sum() - K, U)\n","\n","    pi = Dirichlet(alpha).sample().squeeze()\n","\n","    return torch.log(pi)\n","\n","# pi = update_pi(latent_states, K)\n","# print(np.exp(pi))"]},{"cell_type":"code","execution_count":22,"metadata":{"id":"Bka35usiJlkv","executionInfo":{"status":"ok","timestamp":1649655036650,"user_tz":-480,"elapsed":2,"user":{"displayName":"Chung I Lu","userId":"13493253019536068280"}}},"outputs":[],"source":["def calculate_emission_prob(users_ds, users_Nt, nbd_first_part, multi_first_part, theta, a, b, n_splits=n_splits, splits=splits):\n","    \n","    # log prob of N given z as gamma mixture of poisson i.e. number of articles read\n","#     p_n_ab = gammaln(users_Nt[..., np.newaxis] + a[np.newaxis, np.newaxis, ...]) \\\n","#             - gammaln(a)[np.newaxis, np.newaxis, ...] - gammaln(users_Nt+1)[..., np.newaxis] \\\n","#             + users_Nt[..., np.newaxis] * np.log(b)[np.newaxis, np.newaxis, ...]  \\\n","#             - (users_Nt[..., np.newaxis] + a[np.newaxis, np.newaxis, ...]) * np.log(b+1)[np.newaxis, np.newaxis, ...]\n","\n","    second_part = users_Nt.unsqueeze(-1) * torch.log(b).unsqueeze(0).unsqueeze(0)  \\\n","                - (users_Nt.unsqueeze(-1) + a.unsqueeze(0).unsqueeze(0)) * torch.log(b+1).unsqueeze(0).unsqueeze(0)\n","    \n","    p_n_ab = nbd_first_part + second_part\n","\n","    # log prob of I given z and N as Multinomial(theta) i.e. which movies are rated=1/unrated=0\n","    splits_list = []\n","    for i in range(n_splits):\n","        temp_ds = users_ds[i*splits:(i+1)*splits]\n","        splits_list.append(torch.matmul(temp_ds, theta.T))\n","    temp_ds = users_ds[n_splits * splits:]\n","    splits_list.append(torch.matmul(temp_ds, theta.T))\n","    second_part = torch.cat(splits_list, dim=0)\n","    del temp_ds, splits\n","\n","    p_i_theta = multi_first_part + second_part\n","\n","    # log prob of joint dist of N, I given z\n","    p_i_z = p_n_ab + p_i_theta\n","\n","    return p_i_z, p_n_ab, p_i_theta"]},{"cell_type":"code","source":["# out1 = torch.matmul(users_ds, theta.T)\n","# ds1 = users_ds[:600]\n","# ds2 = users_ds[600:]\n","# out2 = torch.matmul(ds1, theta.T)\n","# out3 = torch.matmul(ds2, theta.T)\n","# out4 = torch.cat((out2, out3), dim=0)\n","# assert (out1 == out4).all()"],"metadata":{"id":"8KKwKkcOaV7x","executionInfo":{"status":"ok","timestamp":1649654965270,"user_tz":-480,"elapsed":14,"user":{"displayName":"Chung I Lu","userId":"13493253019536068280"}}},"execution_count":15,"outputs":[]},{"cell_type":"code","execution_count":16,"metadata":{"id":"jDGNhNhhJlkv","executionInfo":{"status":"ok","timestamp":1649654965271,"user_tz":-480,"elapsed":15,"user":{"displayName":"Chung I Lu","userId":"13493253019536068280"}}},"outputs":[],"source":["def calculate_posterior(A, pi, p_i_z, U, T, K):\n","\n","    # Calculate normalised alpha and beta\n","    alpha = torch.zeros((U,T,K), dtype=torch.float).to(device)\n","    p_i_i = torch.zeros((U,T), dtype=torch.float).to(device)\n","\n","    alpha[:,0] = p_i_z[:,0] + pi\n","    alpha[:,0] -= torch.logsumexp(alpha[:,0], axis=-1).unsqueeze(-1)\n","    for t in range(1, T):\n","        alpha[:,t] = logdotexp(alpha[:,t-1], A) + p_i_z[:,t]\n","        p_i_i[:,t] = torch.logsumexp(alpha[:,t], dim=-1)\n","        alpha[:,t] -= p_i_i[:,t].unsqueeze(-1)\n","\n","    beta = torch.zeros((U,T,K), dtype=torch.float).to(device)\n","    for u in range(U):\n","        for t in range(T-2, -1, -1):\n","            beta[u,t] = logdotexp(A, (p_i_z[u,t+1] + beta[u,t+1]))\n","            beta[u,t] -= p_i_i[u,t+1] # normalization\n","\n","    # numerical issues \"divide by zero encountered in log\" for the vectorized code below\n","    # for t in range(T-2, -1, -1):\n","    #     beta[:,t,:] = logdotexp((p_i_z[:,t+1,:] + beta[:,t+1,:]), A.T)\n","    #     beta[:,t,:] -= p_i_i[:,t+1][...,np.newaxis] # normalization\n","\n","    # log prob of Z(t) given I(1:T)\n","    p_z_i = alpha + beta\n","\n","    # log prob of Z(t), Z(t+1) given I(1:T)\n","    p_zz_i = torch.zeros((U,T-1,K,K), dtype=torch.float).to(device)\n","    for u in range(U):\n","        for t in range(T-1):\n","            p_zz_i[u,t,:,:] = torch.tile(alpha[u,t,:], (K,1)).T + A + torch.tile(p_i_z[u,t+1,:], (K,1)) + torch.tile(beta[u,t+1,:], (K,1)) \n","            p_zz_i[u,t,:,:] -= p_i_i[u,t+1].unsqueeze(-1).unsqueeze(-1) # normalization\n","    \n","    return p_z_i, p_zz_i"]},{"cell_type":"code","execution_count":17,"metadata":{"id":"fvPiumQfJlkv","executionInfo":{"status":"ok","timestamp":1649654965271,"user_tz":-480,"elapsed":15,"user":{"displayName":"Chung I Lu","userId":"13493253019536068280"}}},"outputs":[],"source":["def update_all_states(latent_states, p_z_i, p_zz_i):\n","    U, T = latent_states.shape\n","\n","    sample_pi = torch.exp(p_z_i[:,0,:]).mean(dim=0)    \n","    latent_states[:,0] = torch.multinomial(sample_pi, U, replacement=True)\n","\n","    sample_A = torch.exp(p_zz_i - p_z_i[:,:-1,:].unsqueeze(-1)).mean(dim=0)\n","    for t in range(T-1):\n","        previous_states = latent_states[:,t]\n","        pvals = sample_A[t, previous_states]\n","        latent_states[:,t+1] = torch.multinomial(pvals, 1, replacement=True).squeeze()\n","\n","    return latent_states\n","\n","# p_i_z, p_n_ab, p_i_theta = calculate_emission_prob(users_ds, users_Nt, nbd_first_part, multi_first_part, theta, a, b)\n","# p_z_i, p_zz_i = calculate_posterior(A, pi, p_i_z, U, T, K)\n","# update_all_states(latent_states, p_z_i, p_zz_i)"]},{"cell_type":"code","execution_count":18,"metadata":{"id":"cEExbDLBJlkw","executionInfo":{"status":"ok","timestamp":1649654965271,"user_tz":-480,"elapsed":14,"user":{"displayName":"Chung I Lu","userId":"13493253019536068280"}}},"outputs":[],"source":["def calculate_log_likelihood(pi, A, p_n_ab, p_i_theta, p_z_i, p_zz_i):\n","    # intial state \n","    init = torch.sum(torch.multiply(torch.exp(p_z_i[:,0]), pi.unsqueeze(0)))\n","\n","    # transitional \n","    trans = torch.sum(torch.multiply(torch.exp(p_zz_i), A.unsqueeze(0).unsqueeze(0)))\n","\n","    # # of items \n","    nbd = torch.sum(torch.multiply(torch.exp(p_z_i), p_n_ab))\n","\n","    # specific item \n","    multi = torch.sum(torch.multiply(torch.exp(p_z_i), p_i_theta))\n","    \n","    return init + trans + nbd + multi"]},{"cell_type":"code","source":["# out1 = torch.lgamma(users_ds + 1).sum(dim=-1)\n","# out2 = torch.lgamma(ds1 + 1).sum(dim=-1)\n","# out3 = torch.lgamma(ds2 + 1).sum(dim=-1)\n","# out4 = torch.cat((out2, out3), dim=0)\n","# assert (out1 == out4).all()"],"metadata":{"id":"DkP4GSmUb3LB","executionInfo":{"status":"ok","timestamp":1649654965271,"user_tz":-480,"elapsed":14,"user":{"displayName":"Chung I Lu","userId":"13493253019536068280"}}},"execution_count":19,"outputs":[]},{"cell_type":"markdown","source":["# Gibbs Sampling"],"metadata":{"id":"D9xTLi3IFqcm"}},{"cell_type":"code","execution_count":20,"metadata":{"id":"-fNCbBYWJEJE","executionInfo":{"status":"ok","timestamp":1649654965272,"user_tz":-480,"elapsed":15,"user":{"displayName":"Chung I Lu","userId":"13493253019536068280"}}},"outputs":[],"source":["def gibbs_sampling(users_ds, users_Nt, pi_alpha, A_alpha, theta_alpha, n_iterations, burn_in=50, print_freq=1):\n","    max_likelihood = None\n","    U, T = users_ds.shape[:-1]\n","    K = len(pi_alpha)\n","\n","    pi, A, theta, a, b = initialise_params_and_probs(pi_alpha, A_alpha, theta_alpha)\n","    latent_states = initialise_latent_states(pi, A, U, T)\n","\n","    # store repeated calculations in NBD and multinomial log prob for significant speed up\n","    splits_list = []\n","    for i in range(n_splits):\n","        temp_ds = users_ds[i*splits:(i+1)*splits]\n","        splits_list.append(torch.lgamma(temp_ds + 1).sum(dim=-1))\n","    temp_ds = users_ds[n_splits * splits:]\n","    splits_list.append(torch.lgamma(temp_ds + 1).sum(dim=-1))\n","    multi_first_part = (torch.lgamma(users_Nt + 1) - torch.cat(splits_list, dim=0)).unsqueeze(-1)\n","    del temp_ds, splits_list\n","\n","    nbd_first_part = torch.lgamma(users_Nt.unsqueeze(-1) + a.unsqueeze(0).unsqueeze(0)) \\\n","            - torch.lgamma(a).unsqueeze(0).unsqueeze(0) - torch.lgamma(users_Nt+1).unsqueeze(-1) \n","\n","    # initialise sum of parameters\n","    pi_bar = torch.zeros(pi.shape).to(device)\n","    A_bar = torch.zeros(A.shape).to(device)\n","    theta_bar = torch.zeros(theta.shape).to(device)\n","    b_bar = torch.zeros(b.shape).to(device)\n","\n","    # GIBBS SAMPLING\n","    start_time = datetime.now() # for keeping track of running time\n","    for iteration in range(n_iterations + burn_in):\n","\n","        # UPDATE PARAMETERS AND PROBABILITIES\n","        b = update_b(latent_states, users_Nt, K, a[0])\n","        theta = update_theta(latent_states, users_ds, K)\n","        A = update_A(latent_states, K)\n","        pi = update_pi(latent_states, K)\n","\n","        # UPDATE LATENT STATES    \n","        p_i_z, p_n_ab, p_i_theta = calculate_emission_prob(users_ds, users_Nt, nbd_first_part, multi_first_part, theta, a, b)\n","        p_z_i, p_zz_i = calculate_posterior(A, pi, p_i_z, U, T, K)\n","        latent_states = update_all_states(latent_states, p_z_i, p_zz_i)\n","\n","        # CALCULATE EXPECTED LOG LIKELIHOOD\n","        likelihood = calculate_log_likelihood(pi, A, p_n_ab, p_i_theta, p_z_i, p_zz_i)\n","        if max_likelihood is None: \n","            max_likelihood = likelihood\n","        else:\n","            if likelihood > max_likelihood: \n","                max_likelihood = likelihood\n","                pi_max = pi\n","                A_max = A\n","                theta_max = theta\n","                a_max = a\n","                b_max = b\n","\n","        print('Iteration', iteration+1,': log likelihood =', likelihood)\n","\n","        if iteration+1 > burn_in:\n","            pi_bar += pi\n","            A_bar += A\n","            theta_bar += theta\n","            b_bar += b\n","\n","    pi_bar /= n_iterations\n","    A_bar /= n_iterations\n","    theta_bar /= n_iterations\n","    b_bar /= n_iterations\n","\n","    run_time = datetime.now() - start_time    \n","    print('Execution time for Gibbs Sampling iterations:', run_time)\n","\n","    return ((pi_bar, A_bar, theta_bar, a, b_bar), \n","            (pi_max, A_max, theta_max, a_max, b_max))"]},{"cell_type":"code","execution_count":23,"metadata":{"id":"KOBNEsdZJlkw","outputId":"ee48e34b-27d1-4adc-a1cb-8747715e22f6","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1649655136365,"user_tz":-480,"elapsed":93812,"user":{"displayName":"Chung I Lu","userId":"13493253019536068280"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Iteration 1 : log likelihood = tensor(-14983963., device='cuda:0')\n","Iteration 2 : log likelihood = tensor(-14855959., device='cuda:0')\n","Iteration 3 : log likelihood = tensor(-14609771., device='cuda:0')\n","Execution time for Gibbs Sampling iterations: 0:01:33.303807\n"]}],"source":["n_iterations = 2\n","K = 5\n","\n","prior_const = 0.9*K                                       # affects the parameters of the Dirichlet priors\n","pi_alpha = prior_const/K * torch.ones((K))                 # alpha hyperparams for pi\n","A_alpha = prior_const/K * torch.ones((K,K))                # alpha hyperparams for A\n","theta_alpha = prior_const/K * torch.ones((K,N))            # alpha hyperparams for theta\n","\n","expected_params, max_params = gibbs_sampling(users_ds, users_Nt, pi_alpha, A_alpha, theta_alpha, n_iterations, burn_in=1)\n","pi_bar, A_bar, theta_bar, a_bar, b_bar = expected_params\n","pi_max, A_max, theta_max, a_max, b_max = max_params"]},{"cell_type":"code","source":["# save parameters\n","\n","np.save(path + 'pi_bar_K_' + str(K), pi_bar.cpu().numpy())\n","np.save(path + 'A_bar_K_' + str(K), A_bar.cpu().numpy())\n","np.save(path + 'theta_bar_K_' + str(K), theta_bar.cpu().numpy())\n","np.save(path + 'a_bar_K_' + str(K), a_bar.cpu().numpy())\n","np.save(path + 'b_bar_K_' + str(K), b_bar.cpu().numpy())\n","\n","np.save(path + 'pi_max_K_' + str(K), pi_max.cpu().numpy())\n","np.save(path + 'A_max_K_' + str(K), A_max.cpu().numpy())\n","np.save(path + 'theta_max_K_' + str(K), theta_max.cpu().numpy())\n","np.save(path + 'a_max_K_' + str(K), a_max.cpu().numpy())\n","np.save(path + 'b_max_K_' + str(K), b_max.cpu().numpy())"],"metadata":{"id":"KSw2bp1-q-Bd","executionInfo":{"status":"aborted","timestamp":1649654974828,"user_tz":-480,"elapsed":628,"user":{"displayName":"Chung I Lu","userId":"13493253019536068280"}}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Evaluation on test data"],"metadata":{"id":"aCy2lnK7FsoZ"}},{"cell_type":"code","source":["# Use either the expected parameter values or the max likelihood parameteres\n","\n","# pi, A, theta, a, b = pi_bar, A_bar, theta_bar, a_bar, b_bar\n","\n","pi, A, theta, a, b = pi_max, A_max, theta_max, a_max, b_max"],"metadata":{"id":"rx3qfw1kq9he","executionInfo":{"status":"aborted","timestamp":1649654974830,"user_tz":-480,"elapsed":630,"user":{"displayName":"Chung I Lu","userId":"13493253019536068280"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-mbUoe97Jlkx","executionInfo":{"status":"aborted","timestamp":1649654974830,"user_tz":-480,"elapsed":629,"user":{"displayName":"Chung I Lu","userId":"13493253019536068280"}}},"outputs":[],"source":["multi_first_part = (torch.lgamma(users_Nt + 1) - torch.lgamma(users_ds + 1).sum(dim=-1)).unsqueeze(-1)\n","nbd_first_part = torch.lgamma(users_Nt.unsqueeze(-1) + a.unsqueeze(0).unsqueeze(0)) \\\n","        - torch.lgamma(a).unsqueeze(0).unsqueeze(0) - torch.lgamma(users_Nt+1).unsqueeze(-1) \n","p_i_z, p_n_ab, p_i_theta = calculate_emission_prob(users_ds, users_Nt, nbd_first_part, multi_first_part, theta, a, b)\n","p_z_i, p_zz_i = calculate_posterior(A, pi, p_i_z, U, T, K)\n","likelihood = calculate_log_likelihood(pi, A, p_n_ab, p_i_theta, p_z_i, p_zz_i)\n","print(likelihood)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3dIxUJCJJlkx","executionInfo":{"status":"aborted","timestamp":1649654974830,"user_tz":-480,"elapsed":629,"user":{"displayName":"Chung I Lu","userId":"13493253019536068280"}}},"outputs":[],"source":["# number of items to recommend\n","num_items = 5000\n","\n","# log prob of user each latent class in next period assuming user in Z(t) with log p(Z(t)|I(1:T))\n","# result is multiplying transitional prob to prob of user in each latent class at time t\n","p_z = logdotexp(p_z_i[:,-1], A).cpu().numpy()\n","\n","pi, A, theta, a, b = pi.cpu().numpy(), A.cpu().numpy(), theta.cpu().numpy(), a.cpu().numpy(), b.cpu().numpy() # move back to cpu and numpy\n","\n","# calculate probability that item i is not read in the next time period\n","p_noti_z = np.power(1 + b[...,np.newaxis] * np.exp(theta), -a[...,np.newaxis])\n","\n","# calculate rank score of the items likely to appear in next time period\n","rank_score = -np.exp(p_z) @ p_noti_z\n","\n","# generate indices of top num_items to recommend which will be unsorted\n","rec_list = np.argpartition(rank_score, -num_items, axis=-1)[:,-num_items:]\n","\n","# sort indices by rank score\n","rec_list_score = np.array([row[rec_list[i,:]] for i, row in enumerate(rank_score)]) # get the scores of items in rec_list\n","sorted_rec_list = np.array([row[np.flip(np.argsort(rec_list_score[i]))] for i, row in enumerate(rec_list)]) # sort the rec_list based on the score\n","\n","# check if item in user history\n","user_history = np.array([row[:,sorted_rec_list[i]] for i, row in enumerate(users_ds.cpu().numpy())]) # get all values in user_ds corresponding to the item in rec_list for each user in each time period\n","user_history = np.sum(user_history, axis=1) # get boolean array indicating whether each item in sorted_rec_list is in user history (assumes user only has each item at most once)\n","if user_history.max() > 1: print('There are repeated ratings of a movie by at least one user')\n","# print(user_history.shape)\n","\n","# filter sorted_rec_list for items not in user history\n","filtered_rec_list = [row[np.logical_not(user_history[i])] for i, row in enumerate(sorted_rec_list)] # each user's list will not have the same amount of items as it depends on user history\n","\n","# get multi-hot encoding of top N recommended movies for the next period\n","mlb = MultiLabelBinarizer(classes=range(N), sparse_output=False) # prediction done on based on one hot encoding indexing i.e. starting index is 0\n","top_5_list = [mlb.fit_transform([user[:5]]) for user in filtered_rec_list] # convert top 5 list to one hot encoding\n","top_10_list = [mlb.fit_transform([user[:10]]) for user in filtered_rec_list] \n","\n","# test how many of top N recommended movies appear in user's rated list of movies in the test period\n","positive_top_5 = [np.multiply(test_ds[i], rec_user) for i, rec_user in enumerate(top_5_list)] # get (#users,#items) boolean vectors indicating whether recommended movie was rating in test period\n","users_result_top_5 = [row.sum() for row in positive_top_5] # get list of positive matches per user\n","all_result_top_5 = np.sum(users_result_top_5) # total number of positive matches across all users\n","\n","positive_top_10 = [np.multiply(test_ds[i], rec_user) for i, rec_user in enumerate(top_10_list)] \n","users_result_top_10 = [row.sum() for row in positive_top_10] # get list of positive matches per user\n","all_result_top_10 = np.sum(users_result_top_10) # total number of positive matches across all users\n","\n","# total number of movies rated in test period\n","test_num_movies_rated = np.sum(test_ds).sum()\n","\n","# output results to excel via pandas df\n","dict_result = {'Log Likelihood':likelihood.cpu().numpy(), 'Iterations':n_iterations,            \n","            '# movies rated in test period': test_num_movies_rated, \n","            'Total +ve for top 5':all_result_top_5, \n","            'Precision of top 5':all_result_top_5/(5*U),\n","            'Recall of top 5':all_result_top_5/test_num_movies_rated,\n","            'Total +ve for top 10':all_result_top_10,\n","            'Precision of top 10':all_result_top_10/(10*U),\n","            'Recall of top 10':all_result_top_10/test_num_movies_rated\n","            }\n","df_result = pd.DataFrame(data=dict_result, index=[0])\n","print(df_result)\n","df_result.to_csv(path + 'result.csv', index=False)"]}]}