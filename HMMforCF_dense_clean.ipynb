{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"HMMforCF_dense_clean.ipynb","provenance":[{"file_id":"11SWLML91N1kIcylqpfhbaOq12AIy6ugf","timestamp":1634616253864},{"file_id":"18mm9Pcn74YYmTpccJy-lWPF3M5ByrfbO","timestamp":1633762713237},{"file_id":"1v7vqEVkzjGbWDMR6Et_yXqG8OZv4ovdS","timestamp":1633758281591},{"file_id":"1c5sPR_6ZnojLorUgql_o_ZrjRawNQf_S","timestamp":1631338387917}],"collapsed_sections":[],"machine_shape":"hm"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.10"}},"cells":[{"cell_type":"markdown","metadata":{"id":"78tSnQTojYSg"},"source":["# Packages and Functions"]},{"cell_type":"code","metadata":{"id":"fEyqx4aCkAi1","executionInfo":{"status":"ok","timestamp":1649730878451,"user_tz":-480,"elapsed":1119,"user":{"displayName":"Chung I Lu","userId":"13493253019536068280"}}},"source":["import numpy as np\n","import pandas as pd\n","from scipy.stats import dirichlet\n","from scipy.special import logsumexp, gammaln, digamma, polygamma\n","from datetime import datetime\n","import pickle\n","from sklearn.preprocessing import MultiLabelBinarizer"],"execution_count":1,"outputs":[]},{"cell_type":"code","metadata":{"id":"v9uY1aJSfy4J","executionInfo":{"status":"ok","timestamp":1649730878452,"user_tz":-480,"elapsed":3,"user":{"displayName":"Chung I Lu","userId":"13493253019536068280"}}},"source":["def logdotexp(A, B):\n","    max_A = np.max(A)\n","    max_B = np.max(B)\n","    C = np.dot(np.exp(A - max_A), np.exp(B - max_B))\n","    np.log(C, out=C)\n","    C += max_A + max_B\n","    return C"],"execution_count":2,"outputs":[]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"X5mMGB0wn5QA","executionInfo":{"status":"ok","timestamp":1649730905803,"user_tz":-480,"elapsed":19020,"user":{"displayName":"Chung I Lu","userId":"13493253019536068280"}},"outputId":"012cb918-865c-42cc-e3fd-0943eba24f00"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"markdown","metadata":{"id":"jJWglv0Hi0Xd"},"source":["# Load Dataset (Dense)"]},{"cell_type":"code","metadata":{"id":"wcnnSVuQi4L_","executionInfo":{"status":"ok","timestamp":1649730949352,"user_tz":-480,"elapsed":11324,"user":{"displayName":"Chung I Lu","userId":"13493253019536068280"}}},"source":["# path for loading dataset\n","path = '/content/drive/MyDrive/PhD/Modules/CS5340 Uncertainty Modeling in AI/Project/'\n","users_ds = np.load(path + 'users_ds_dense.npy', allow_pickle=True)"],"execution_count":4,"outputs":[]},{"cell_type":"code","metadata":{"id":"SW9OGXCgoMgZ"},"source":["# test_ds = [user[-1,:] for user in users_ds]\n","# users_ds = [user[:-1,:] for user in users_ds]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"oqAzxZEiio3M","executionInfo":{"status":"ok","timestamp":1649730949353,"user_tz":-480,"elapsed":5,"user":{"displayName":"Chung I Lu","userId":"13493253019536068280"}}},"source":["t_predict = -1                                          # index for holdout period for prediction\n","test_ds = users_ds[:,t_predict,:]                       # extract holdout period from dataset\n","test_ds = test_ds[:,np.newaxis,:]                       # align # of dimensions\n","users_ds = users_ds[:,:t_predict,:]                     # specify training periods"],"execution_count":5,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"wJ6gmTioo1Ra","executionInfo":{"elapsed":4,"status":"ok","timestamp":1649730949353,"user":{"displayName":"Chung I Lu","userId":"13493253019536068280"},"user_tz":-480},"outputId":"ff8d2405-06c0-4611-e8d7-d1543d24a139"},"source":["# size of test dataset\n","test_ds.shape "],"execution_count":6,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(1212, 1, 17768)"]},"metadata":{},"execution_count":6}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"tbSrH8FQovkO","executionInfo":{"elapsed":3,"status":"ok","timestamp":1649730949945,"user":{"displayName":"Chung I Lu","userId":"13493253019536068280"},"user_tz":-480},"outputId":"7dd23732-4121-4806-e30d-43d14d15c3c1"},"source":["# size of training dataset\n","U = len(users_ds)\n","T = users_ds[0].shape[0]\n","N = users_ds[0].shape[1]\n","U, T, N"],"execution_count":7,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(1212, 72, 17768)"]},"metadata":{},"execution_count":7}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"HSfcTrkj06AF","executionInfo":{"elapsed":1567,"status":"ok","timestamp":1649730952517,"user":{"displayName":"Chung I Lu","userId":"13493253019536068280"},"user_tz":-480},"outputId":"509ce864-b776-45f3-f61c-4dbfc609647a"},"source":["# calculate and store total number of ratings per user in a period\n","users_Nt = np.sum(users_ds, axis=-1)    # number of movies rated by user in each time period\n","T = np.shape(users_ds)[1]               # final length of time period after trimming\n","users_Nt.shape "],"execution_count":8,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(1212, 72)"]},"metadata":{},"execution_count":8}]},{"cell_type":"markdown","metadata":{"id":"iOLPcJ7fpnBR"},"source":["# Multiple runs on same settings"]},{"cell_type":"code","metadata":{"id":"yYpo4SvIjWxh","outputId":"3579689a-a689-4af6-fd5b-e82105fc2dc3","colab":{"base_uri":"https://localhost:8080/","height":317},"executionInfo":{"status":"error","timestamp":1649731151409,"user_tz":-480,"elapsed":106156,"user":{"displayName":"Chung I Lu","userId":"13493253019536068280"}}},"source":["# path for saving the final parameters in .npy format\n","# path = '/content/drive/MyDrive/PhD/Modules/IS6101 Topics in Machine Learning and Optimization/HMM for CF/Data and Parameters/Parameters/'\n","# path for excel file to save results in table\n","# table_path = '/content/drive/MyDrive/PhD/Modules/IS6101 Topics in Machine Learning and Optimization/HMM for CF/Report/Table.xlsx'\n","\n","# Initial parameters\n","epsilon = 1e-4          # log likelihood convergence epsilon\n","K = 5                  # no of latent classes\n","prior_const = 0.9*K     # affects the parameters of the Dirichlet priors\n","\n","for r in range(1):\n","    pi_alpha = prior_const/K * np.ones((K))             # hyperparams for pi\n","    A_alpha = prior_const/K * np.ones((K,K))            # hyperparams for A\n","    theta_alpha = prior_const/K * np.ones((K,N))        # hyperparams for theta\n","    a = np.random.uniform(low=2, high=5, size=(K))      # initialise parameter a randomly\n","    p = np.random.uniform(low=0.6, high=0.8, size=(K))  # initialise parameter p where p = b / b+1\n","    b = p / (1-p)                                       # derive parameter b as defined in paper\n","    pi = dirichlet.rvs(alpha=pi_alpha, size=1)          # initialise pi randomly\n","    A = np.zeros((K,K)) \n","    theta = np.zeros((K,N))\n","    for k in range(K):\n","        A[k,:] = dirichlet.rvs(alpha=A_alpha[k,:], size=1)          # initialise matrix A row by row\n","        theta[k,:] = dirichlet.rvs(alpha=theta_alpha[k,:], size=1)  # initialise theta row by row\n","\n","    # convert parameters all to log probabilities\n","    A = np.log(A)\n","    pi = np.log(pi)\n","    theta = np.log(theta)\n","\n","    # EM Algorithm\n","    start_time = datetime.now() # for keeping track of running time\n","    a_epsilon = 1e-3            # convergence threshold for optimising parameter a (max 10 iterations)\n","    old_likelihood = None       # keep track of previous iteration log likelihood \n","    iteration = 0               # keep track of number of itrations\n","    first_part = (gammaln(users_Nt + 1) - gammaln(users_ds + 1).sum(axis=-1))[..., np.newaxis]  # store repeated calculations in multinomial log prob for significant speed up\n","    while True:\n","        # E STEP\n","        \n","        # log prob of N given z as gamma mixture of poisson i.e. number of articles read\n","        p_n_ab = gammaln(users_Nt[..., np.newaxis] + a[np.newaxis, np.newaxis, ...]) \\\n","                - gammaln(a)[np.newaxis, np.newaxis, ...] - gammaln(users_Nt+1)[..., np.newaxis] \\\n","                + users_Nt[..., np.newaxis] * np.log(b)[np.newaxis, np.newaxis, ...]  \\\n","                - (users_Nt[..., np.newaxis] + a[np.newaxis, np.newaxis, ...]) * np.log(b+1)[np.newaxis, np.newaxis, ...]\n","\n","        # log prob of I given z and N as Multinomial(theta) i.e. which articles are read where read=1/unread=0\n","        second_part = np.dot(users_ds, theta.T)\n","        p_i_theta = first_part + second_part\n","        del second_part\n","\n","        # log prob of joint dist of N, I given z\n","        p_i_z = p_n_ab + p_i_theta\n","\n","        # HMM for CF paper definition of alpha and beta\n","        alpha = np.empty((U,T,K), dtype='float64')\n","        p_i_i = np.empty((U,T), dtype='float64')\n","\n","        alpha[:,0] = p_i_z[:,0] + pi\n","        alpha[:,0] -= logsumexp(alpha[:,0], axis=-1)[...,np.newaxis]\n","        for t in range(1, T):\n","            alpha[:,t] = logdotexp(alpha[:,t-1], A) + p_i_z[:,t]\n","            p_i_i[:,t] = logsumexp(alpha[:,t], axis=-1)\n","            alpha[:,t] -= p_i_i[:,t][...,np.newaxis]\n","\n","        beta = np.zeros((U,T,K), dtype='float64')\n","        for u in range(U):\n","            for t in range(T-2, -1, -1):\n","                beta[u,t] = logdotexp(A, (p_i_z[u,t+1] + beta[u,t+1]))\n","                beta[u,t] -= p_i_i[u,t+1] # normalization\n","\n","        # numerical issues \"divide by zero encountered in log\" for the vectorized code below\n","        # for t in range(T-2, -1, -1):\n","        #     beta[:,t,:] = logdotexp((p_i_z[:,t+1,:] + beta[:,t+1,:]), A.T)\n","        #     beta[:,t,:] -= p_i_i[:,t+1][...,np.newaxis] # normalization\n","\n","        # log prob of Z(t) given I(1:T)\n","        p_z_i = alpha + beta\n","\n","        # log prob of Z(t), Z(t+1) given I(1:T)\n","        p_zz_i = np.empty((U,T-1,K,K), dtype='float64')\n","        for u in range(U):\n","            for t in range(T-1):\n","                p_zz_i[u,t,:,:] = np.tile(alpha[u,t,:], (K,1)).T + A + np.tile(p_i_z[u,t+1,:], (K,1)) + np.tile(beta[u,t+1,:], (K,1)) \n","                p_zz_i[u,t,:,:] -= p_i_i[u,t+1][...,np.newaxis,np.newaxis] # normalization\n","\n","        del alpha\n","        del beta\n","        del p_i_i\n","        del p_i_z\n","\n","        # CALCULATE EXPECTED LOG LIKELIHOOD\n","        # check that it is increasing at every iteration and check for convergence condition\n","\n","        # intial state \n","        init = np.sum(np.multiply(np.exp(p_z_i[:,0]), pi[np.newaxis,...]))\n","\n","        # transitional \n","        trans = np.sum(np.multiply(np.exp(p_zz_i), A[np.newaxis,np.newaxis,...]))\n","\n","        # # of items \n","        nbd = np.sum(np.multiply(np.exp(p_z_i), p_n_ab))\n","\n","        # specific item \n","        multi = np.sum(np.multiply(np.exp(p_z_i), p_i_theta))\n","\n","        if old_likelihood is None:\n","            old_likelihood = init + trans + nbd + multi\n","        else:\n","            new_likelihood = init + trans + nbd + multi \n","\n","            if np.isnan(new_likelihood):\n","                print('Numerical issues in calculation of log likelihood\\nPrevious calculated log likelihood = ', old_likelihood)\n","                break\n","            if new_likelihood < old_likelihood:\n","                print('Iteration resulted in lower log likelihood =', new_likelihood)\n","                break\n","            if np.abs((new_likelihood - old_likelihood) / old_likelihood) < epsilon:\n","                old_likelihood = new_likelihood\n","                print('Iteration', iteration,': log likelihood =', old_likelihood, '\\n')\n","                print('Convergence attained')\n","                break\n","            old_likelihood = new_likelihood\n","        print('Iteration', iteration,': log likelihood =', old_likelihood)            \n","        iteration += 1\n","\n","        del p_n_ab\n","        del p_i_theta\n","\n","        # M STEP\n","\n","        # update pi using MAP\n","        pi_alpha += np.sum(np.exp(p_z_i[:,0]), axis=0)\n","        if pi_alpha.min() < 1:      # check to ensure no negative probabilities will result\n","            pi = pi_alpha / np.sum(pi_alpha)\n","        else:\n","            pi = (pi_alpha - 1) / (np.sum(pi_alpha) - K)\n","        pi = np.log(pi)\n","\n","        # update A using MAP\n","        A_alpha += np.sum(np.exp(p_zz_i), axis=(0,1))\n","        if A_alpha.min() < 1:       # check to ensure no negative probabilities will result\n","            A = A_alpha / np.sum(A_alpha, axis=-1)[...,np.newaxis] # to align the division\n","        else:\n","            A = (A_alpha - 1) / (np.sum(A_alpha, axis=-1) - K)[...,np.newaxis] # align the division\n","        A = np.log(A)\n","\n","        # update theta using MAP\n","        # theta_alpha += np.sum(np.multiply(users_ds[:,:,np.newaxis,:], np.exp(p_z_i[...,np.newaxis])), axis=(0,1)) # uses too much memory\n","        for k in range(K):\n","            theta_alpha[k,:] += np.sum(np.multiply(users_ds, np.exp(p_z_i[:,:,k][...,np.newaxis])), axis=(0,1))\n","        if theta_alpha.min() < 1:\n","            theta = theta_alpha / (np.sum(theta_alpha, axis=-1))[...,np.newaxis]\n","        else:\n","            theta = (theta_alpha - 1) / ((np.sum(theta_alpha, axis=-1)) - K)[...,np.newaxis]\n","        theta = np.log(theta)\n","        \n","        # update a using MLE with Newton's method\n","        for _ in range(10):     \n","            N_bar = np.sum((np.exp(p_z_i) * users_Nt[...,np.newaxis] + a[np.newaxis,np.newaxis,...]) * \n","                        (b / (b + 1))[np.newaxis,np.newaxis,...], axis=(0,1)) / (U*T)\n","            log_N_bar = np.sum(digamma(np.exp(p_z_i) * users_Nt[...,np.newaxis] + a[np.newaxis,np.newaxis,...]) + \n","                            np.log(b / (b + 1))[np.newaxis,np.newaxis,...], axis=(0,1)) / (U*T)\n","            a_new = (1/a + (log_N_bar - np.log(N_bar) + np.log(a) - digamma(a)) / (a**2 * (1/a - polygamma(1, a))))**-1\n","            if np.isnan(a).any():\n","                print('Numerical issues in calculating parameter a')\n","                break\n","            if np.sum(np.abs(a_new - a)) / np.sum(a) < a_epsilon:\n","                a = a_new\n","                b = N_bar / a  \n","                break\n","            a = a_new\n","\n","        # update b using MLE \n","        b = N_bar / a      \n","\n","    run_time = datetime.now() - start_time\n","    np.set_printoptions(precision=3)\n","    print('Execution Time:', run_time)\n","    print('\\npi = ', np.exp(pi),'\\n\\nA = ', np.exp(A),'\\n\\na = ', a,'\\n\\nb = ', b,'\\n\\np(Z(t=T, u=0:10)|I) = \\n', np.exp(p_z_i[:3,-1]))\n","    # save parameters\n","    threshold = '2000'\n","    alpha = str(prior_const)\n","    run = str(r+1)    \n","    # np.save(path + 'pi_K_' + str(K) + '_threshold_' + threshold + '_alpha_' + alpha + '_run_' + run, pi)\n","    # np.save(path + 'mA_K_' + str(K) + '_threshold_' + threshold + '_alpha_' + alpha + '_run_' + run, A)\n","    # np.save(path + 'va_K_' + str(K) + '_threshold_' + threshold + '_alpha_' + alpha + '_run_' + run, a)\n","    # np.save(path + 'b_K_' + str(K) + '_threshold_' + threshold + '_alpha_' + alpha + '_run_' + run, b)\n","    # np.save(path + 'user_class_K_' + str(K) + '_threshold_' + threshold + '_alpha_' + alpha + '_run_' + run, p_z_i[:,-1])\n","\n","    # number of items to recommend\n","    num_items = 5000\n","\n","    # log prob of user each latent class in next period assuming user in Z(t) with log p(Z(t)|I(1:T))\n","    # result is multiplying transitional prob to prob of user in each latent class at time t\n","    p_z = logdotexp(p_z_i[:,-1], A)\n","\n","    # calculate probability that item i is not read in the next time period\n","    p_noti_z = np.power(1 + b[...,np.newaxis] * np.exp(theta), -a[...,np.newaxis])\n","\n","    # calculate rank score of the items likely to appear in next time period\n","    rank_score = -np.exp(p_z) @ p_noti_z\n","\n","    # generate indices of top num_items to recommend which will be unsorted\n","    rec_list = np.argpartition(rank_score, -num_items, axis=-1)[:,-num_items:]\n","\n","    # sort indices by rank score\n","    rec_list_score = np.array([row[rec_list[i,:]] for i, row in enumerate(rank_score)]) # get the scores of items in rec_list\n","    sorted_rec_list = np.array([row[np.flip(np.argsort(rec_list_score[i]))] for i, row in enumerate(rec_list)]) # sort the rec_list based on the score\n","\n","    # check if item in user history\n","    user_history = np.array([row[:,sorted_rec_list[i]] for i, row in enumerate(users_ds)]) # get all values in user_ds corresponding to the item in rec_list for each user in each time period\n","    user_history = np.sum(user_history, axis=1) # get boolean array indicating whether each item in sorted_rec_list is in user history (assumes user only has each item at most once)\n","    if user_history.max() > 1: print('There are repeated ratings of a movie by at least one user')\n","    # print(user_history.shape)\n","\n","    # filter sorted_rec_list for items not in user history\n","    filtered_rec_list = [row[np.logical_not(user_history[i])] for i, row in enumerate(sorted_rec_list)] # each user's list will not have the same amount of items as it depends on user history\n","    \n","    # get multi-hot encoding of top N recommended movies for the next period\n","    mlb = MultiLabelBinarizer(range(N), sparse_output=False) # prediction done on based on one hot encoding indexing i.e. starting index is 0\n","    top_5_list = [mlb.fit_transform([user[:5]]) for user in filtered_rec_list] # convert top 5 list to one hot encoding\n","    top_10_list = [mlb.fit_transform([user[:10]]) for user in filtered_rec_list] \n","\n","    # test how many of top N recommended movies appear in user's rated list of movies in the test period\n","    positive_top_5 = [np.multiply(test_ds[i], rec_user) for i, rec_user in enumerate(top_5_list)] # get (#users,#items) boolean vectors indicating whether recommended movie was rating in test period\n","    users_result_top_5 = [row.sum() for row in positive_top_5] # get list of positive matches per user\n","    all_result_top_5 = np.sum(users_result_top_5) # total number of positive matches across all users\n","\n","    positive_top_10 = [np.multiply(test_ds[i], rec_user) for i, rec_user in enumerate(top_10_list)] \n","    users_result_top_10 = [row.sum() for row in positive_top_10] # get list of positive matches per user\n","    all_result_top_10 = np.sum(users_result_top_10) # total number of positive matches across all users\n","\n","    # total number of movies rated in test period\n","    test_num_movies_rated = np.sum(test_ds).sum()\n","\n","    # output results to excel via pandas df\n","    dict_result = {'# Ratings Threshold':threshold, '# Users':U, '# Movies':N, 'K':K, 'T': T, 't_predict':t_predict,\n","                'Dirichlet Prior Parameter':alpha, 'Run':run, 'Convergence epsilon': epsilon,\n","                'Log Likelihood':old_likelihood, 'Iterations':iteration,'Time (min)':run_time*24*60,\n","                'Avg Time per iteration (s)':run_time*24*60*60/iteration, \n","                '# movies rated in test period': test_num_movies_rated, \n","                'Total +ve for top 5':all_result_top_5, \n","                'Precision of top 5':all_result_top_5/(5*U),\n","                'Recall of top 5':all_result_top_5/test_num_movies_rated,\n","                'Total +ve for top 10':all_result_top_10,\n","                'Precision of top 10':all_result_top_10/(10*U),\n","                'Recall of top 10':all_result_top_10/test_num_movies_rated\n","                }\n","    df_result = pd.DataFrame(data=dict_result, index=[0])\n","    print(df_result)\n","\n","    # save result in excel file\n","    # df_table = pd.read_excel(table_path)\n","    # df_table = pd.concat([df_table ,df_result], ignore_index=True)\n","    # df_table.to_excel(table_path, index=False)"],"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["Iteration 0 : log likelihood = -19495519.206469715\n","Iteration 1 : log likelihood = -14457876.151509166\n","Iteration 2 : log likelihood = -14141379.595077662\n"]},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-9-8e0bd0d6b220>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    144\u001b[0m         \u001b[0;31m# theta_alpha += np.sum(np.multiply(users_ds[:,:,np.newaxis,:], np.exp(p_z_i[...,np.newaxis])), axis=(0,1)) # uses too much memory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mK\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 146\u001b[0;31m             \u001b[0mtheta_alpha\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmultiply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0musers_ds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp_z_i\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m...\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnewaxis\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    147\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtheta_alpha\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m             \u001b[0mtheta\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtheta_alpha\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtheta_alpha\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m...\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnewaxis\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"cell_type":"code","metadata":{"id":"hULXjXME7ADG"},"source":[""],"execution_count":null,"outputs":[]}]}