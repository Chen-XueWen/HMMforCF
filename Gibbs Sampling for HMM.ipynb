{"cells":[{"cell_type":"markdown","metadata":{"id":"xoM7Nun63R6s"},"source":["# Packages"]},{"cell_type":"code","execution_count":1,"metadata":{"_cell_guid":"b1f10f39-91d4-45f1-b62f-d1f590e50438","_kg_hide-input":false,"_kg_hide-output":false,"_uuid":"7edd510ba8ac857514e34d6b38c0466d125cffb9","id":"ICksXUZs-3KQ","trusted":true},"outputs":[],"source":["import numpy as np\n","import pandas as pd\n","import pickle\n","import matplotlib.pyplot as plt\n","import seaborn as sb\n","\n","from datetime import datetime\n","\n","from scipy.special import logsumexp, gammaln, digamma, polygamma\n","from scipy.sparse import csr_matrix\n","\n","from sklearn.preprocessing import MultiLabelBinarizer\n","\n","import line_profiler\n","\n","%matplotlib inline\n","%load_ext line_profiler\n","\n","try:\n","    import google.colab\n","    IN_COLAB = True\n","    path = '/content/drive/MyDrive/PhD/Modules/CS5340 Uncertainty Modeling in AI/Project/'\n","except:    \n","    IN_COLAB = False\n","    path = './'\n"]},{"cell_type":"markdown","metadata":{"id":"gldC8sScQBTB"},"source":["# Dataset"]},{"cell_type":"code","execution_count":2,"metadata":{"id":"mUmFmssxKl22"},"outputs":[],"source":["users_ds = np.load(path + 'users_ds_dense.npy', allow_pickle=True)"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4,"status":"ok","timestamp":1649475426876,"user":{"displayName":"Chung I Lu","userId":"13493253019536068280"},"user_tz":-480},"id":"K1QbMOKwKvUM","outputId":"c62c499f-86cd-49ff-e711-62849affe841"},"outputs":[{"name":"stdout","output_type":"stream","text":["Shape of dataset: (1212, 73, 17768)\n","Number of users: 1212\n","Number of movie titles: 17768\n","Number of periods: 73\n"]}],"source":["# size of training dataset\n","U, T, N = users_ds.shape\n","print('Shape of dataset:', users_ds.shape)\n","print('Number of users:', U)\n","print('Number of movie titles:', N)\n","print('Number of periods:', T)"]},{"cell_type":"code","execution_count":4,"metadata":{"id":"AOn8l1KrLUvk"},"outputs":[],"source":["t_predict = -1                                          # index for holdout period for prediction\n","test_ds = users_ds[:,t_predict,:]                       # extract holdout period from dataset\n","test_ds = test_ds[:,np.newaxis,:]                       # align # of dimensions\n","users_ds = users_ds[:,:t_predict,:]                     # specify training periods"]},{"cell_type":"code","execution_count":5,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1424,"status":"ok","timestamp":1649474737539,"user":{"displayName":"Chung I Lu","userId":"13493253019536068280"},"user_tz":-480},"id":"wUj2_ruSK4Op","outputId":"4bd0c39f-dcb2-42ea-acda-ae1d3fdc03d6"},"outputs":[{"data":{"text/plain":["(1212, 72)"]},"execution_count":5,"metadata":{},"output_type":"execute_result"}],"source":["# calculate and store total number of ratings per user in a period\n","users_Nt = np.sum(users_ds, axis=-1)    # number of movies rated by user in each time period\n","T = np.shape(users_ds)[1]               # final length of time period after trimming\n","users_Nt.shape "]},{"cell_type":"markdown","metadata":{"id":"J0NziYIX4hKG"},"source":["# Functions"]},{"cell_type":"code","execution_count":6,"metadata":{"id":"UVivGhN2_qDG"},"outputs":[],"source":["rng = np.random.default_rng()"]},{"cell_type":"code","execution_count":38,"metadata":{"id":"s5vA1jv3KiYk"},"outputs":[],"source":["def logdotexp(A, B):\n","    max_A = np.max(A)\n","    max_B = np.max(B)\n","    C = np.matmul(np.exp(A - max_A), np.exp(B - max_B))\n","    np.log(C, out=C)\n","    C += max_A + max_B\n","    return C"]},{"cell_type":"code","execution_count":8,"metadata":{"id":"DMq-W5vm4zg1"},"outputs":[],"source":["def initialise_params_and_probs(pi_alpha, A_alpha, theta_alpha):\n","    K = len(pi_alpha)\n","    pi = rng.dirichlet(alpha=pi_alpha, size=1).squeeze()                        # Initial state distribution\n","    A = np.zeros((K,K))                                                         # Transitional probabilities\n","    theta = np.zeros((K,N))                                                     # Multinomial probabilities\n","    for k in range(K):\n","        A[k,:] = rng.dirichlet(alpha=A_alpha[k,:], size=1).squeeze()\n","        theta[k,:] = rng.dirichlet(alpha=theta_alpha[k,:], size=1).squeeze()\n","    pi = np.log(pi)\n","    A = np.log(A)\n","    theta = np.log(theta)\n","\n","    # NBD parameters\n","    a = np.ones((K))\n","    p = rng.uniform(low=0.7, high=0.9, size=(K))  # probability for event tied to N\n","    b = p / (1-p)\n","    return pi, A, theta, a, b"]},{"cell_type":"code","execution_count":9,"metadata":{"id":"DXRIyzEvStjB"},"outputs":[],"source":["def initialise_latent_states(pi, A, U, T):\n","    # latent states of all users across the periods\n","    latent_states = np.empty((U,T), dtype=int)\n","    latent_states[:,0] = rng.multinomial(n=1, pvals=np.exp(pi), size=(U,)).argmax(axis=1)    # initial latent state assignments\n","    sample_A = np.tile(np.exp(A), (U,1,1))\n","    for t in range(T-1):\n","        previous_states = latent_states[:,t]\n","        pvals = sample_A[t, previous_states] \n","        latent_states[:,t+1] = rng.multinomial(n=1, pvals=pvals).argmax(axis=1) \n","        \n","    return latent_states\n","\n","# latent_states = initialise_latent_states(pi, A, U, T)\n","# print(latent_states.shape, latent_states[0])"]},{"cell_type":"code","execution_count":10,"metadata":{"id":"Gvlq58JobkgD"},"outputs":[],"source":["def update_b(latent_states, users_Nt, K, a):\n","    alpha = np.ones((K), dtype=int)\n","    beta = users_Nt.shape[0] * users_Nt.shape[1] + 1\n","\n","    b = np.zeros((K))\n","    for i in range(K):\n","        indices = latent_states == i\n","        alpha[i] += users_Nt[indices].sum()\n","        p = rng.beta(alpha[i], beta, 1)\n","        b[i] = p / (1-p)\n","\n","    return b\n","\n","# b = update_b(latent_states, users_Nt, K, 1)\n","# print(b)"]},{"cell_type":"code","execution_count":11,"metadata":{"id":"HIJkmnIuYb2L"},"outputs":[],"source":["def update_theta(latent_states, dataset, K):\n","    alpha = np.ones((K,N), dtype=int)\n","    U = latent_states.shape[0]\n","    for u in range(U):\n","        user_states = latent_states[u,:]\n","        counts = dataset[u,:,:]\n","        for t in range(T):\n","            alpha[user_states[t]] += counts[t]\n","\n","    # print(alpha.sum() - K*N, dataset.sum()) # checksum\n","\n","    # theta = rng.dirichlet(alpha=alpha, size=1) # dirichlet does not accept array like for alpha unlike multinomial\n","\n","    theta = np.zeros((K,N))\n","    for k in range(K):\n","        theta[k,:] = rng.dirichlet(alpha=alpha[k,:], size=1).squeeze()\n","\n","    return np.log(theta)\n","\n","# theta = update_theta(latent_states, users_ds, K)\n","# np.exp(theta).sum(axis=-1)"]},{"cell_type":"code","execution_count":12,"metadata":{"id":"RTU3Y4H1SryC"},"outputs":[],"source":["def update_A(latent_states, K):\n","    alpha = np.ones((K,K), dtype=int)\n","    T = latent_states.shape[1]\n","\n","    for t in range(T - 1):\n","        current_states = latent_states[:,t]\n","        next_states = latent_states[:,t+1]\n","        for i in range(len(current_states)):\n","            alpha[current_states[i], next_states[i]] += 1\n","    \n","    # print(alpha.sum() - K*K, U*(T-1)) # checksum\n","\n","    A = np.zeros((K,K))\n","    for k in range(K):\n","        A[k,:] = rng.dirichlet(alpha=A_alpha[k,:], size=1).squeeze()\n","    return np.log(A)\n","\n","# A = update_A(latent_states, K)\n","# np.exp(A)"]},{"cell_type":"code","execution_count":13,"metadata":{"id":"SX650QntwHYL"},"outputs":[],"source":["def update_pi(latent_states, K):\n","    alpha = np.ones((K), dtype=int)\n","\n","    for i in range(K):\n","        alpha[i] += (latent_states[:,0] == i).sum()\n","\n","    # print(alpha.sum() - K, U)\n","\n","    pi = rng.dirichlet(alpha=alpha, size=1).squeeze()\n","\n","    return np.log(pi)\n","\n","# pi = update_pi(latent_states, K)\n","# print(np.exp(pi))"]},{"cell_type":"code","execution_count":39,"metadata":{"id":"FFiBGUUrDvT1"},"outputs":[],"source":["def calculate_emission_prob(users_ds, users_Nt, nbd_first_part, multi_first_part, theta, a, b):\n","    \n","    # log prob of N given z as gamma mixture of poisson i.e. number of articles read\n","#     p_n_ab = gammaln(users_Nt[..., np.newaxis] + a[np.newaxis, np.newaxis, ...]) \\\n","#             - gammaln(a)[np.newaxis, np.newaxis, ...] - gammaln(users_Nt+1)[..., np.newaxis] \\\n","#             + users_Nt[..., np.newaxis] * np.log(b)[np.newaxis, np.newaxis, ...]  \\\n","#             - (users_Nt[..., np.newaxis] + a[np.newaxis, np.newaxis, ...]) * np.log(b+1)[np.newaxis, np.newaxis, ...]\n","\n","    second_part = users_Nt[..., np.newaxis] * np.log(b)[np.newaxis, np.newaxis, ...]  \\\n","                - (users_Nt[..., np.newaxis] + a[np.newaxis, np.newaxis, ...]) * np.log(b+1)[np.newaxis, np.newaxis, ...]\n","    \n","    p_n_ab = nbd_first_part + second_part\n","\n","    # log prob of I given z and N as Multinomial(theta) i.e. which movies are rated=1/unrated=0\n","    second_part = np.matmul(users_ds, theta.T)\n","    p_i_theta = multi_first_part + second_part\n","    del second_part\n","\n","    # log prob of joint dist of N, I given z\n","    p_i_z = p_n_ab + p_i_theta\n","\n","    return p_i_z, p_n_ab, p_i_theta"]},{"cell_type":"code","execution_count":15,"metadata":{"id":"px5v5K1hDvT2"},"outputs":[],"source":["def calculate_posterior(A, pi, p_i_z, U, T, K):\n","\n","    # Calculate normalised alpha and beta\n","    alpha = np.empty((U,T,K), dtype='float64')\n","    p_i_i = np.empty((U,T), dtype='float64')\n","\n","    alpha[:,0] = p_i_z[:,0] + pi\n","    alpha[:,0] -= logsumexp(alpha[:,0], axis=-1)[...,np.newaxis]\n","    for t in range(1, T):\n","        alpha[:,t] = logdotexp(alpha[:,t-1], A) + p_i_z[:,t]\n","        p_i_i[:,t] = logsumexp(alpha[:,t], axis=-1)\n","        alpha[:,t] -= p_i_i[:,t][...,np.newaxis]\n","\n","    beta = np.zeros((U,T,K), dtype='float64')\n","    for u in range(U):\n","        for t in range(T-2, -1, -1):\n","            beta[u,t] = logdotexp(A, (p_i_z[u,t+1] + beta[u,t+1]))\n","            beta[u,t] -= p_i_i[u,t+1] # normalization\n","\n","    # numerical issues \"divide by zero encountered in log\" for the vectorized code below\n","    # for t in range(T-2, -1, -1):\n","    #     beta[:,t,:] = logdotexp((p_i_z[:,t+1,:] + beta[:,t+1,:]), A.T)\n","    #     beta[:,t,:] -= p_i_i[:,t+1][...,np.newaxis] # normalization\n","\n","    # log prob of Z(t) given I(1:T)\n","    p_z_i = alpha + beta\n","\n","    # log prob of Z(t), Z(t+1) given I(1:T)\n","    p_zz_i = np.empty((U,T-1,K,K), dtype='float64')\n","    for u in range(U):\n","        for t in range(T-1):\n","            p_zz_i[u,t,:,:] = np.tile(alpha[u,t,:], (K,1)).T + A + np.tile(p_i_z[u,t+1,:], (K,1)) + np.tile(beta[u,t+1,:], (K,1)) \n","            p_zz_i[u,t,:,:] -= p_i_i[u,t+1][...,np.newaxis,np.newaxis] # normalization\n","    \n","    return p_z_i, p_zz_i"]},{"cell_type":"code","execution_count":16,"metadata":{"id":"G6UZg3p0DvT1"},"outputs":[],"source":["def update_all_states(latent_states, p_z_i, p_zz_i):\n","    U, T = latent_states.shape\n","\n","    sample_pi = np.exp(p_z_i[:,0,:]).mean(axis=0)    \n","    latent_states[:,0] = rng.multinomial(n=1, pvals=sample_pi, size=(U,)).argmax(axis=1)\n","\n","    sample_A = np.exp(p_zz_i - p_z_i[:,:-1,:][...,np.newaxis]).mean(axis=(0))\n","    for t in range(T-1):\n","        previous_states = latent_states[:,t]\n","        pvals = sample_A[t, previous_states]\n","        latent_states[:,t+1] = rng.multinomial(n=1, pvals=pvals).argmax(axis=1)\n","\n","    # unnormalised_pi = pi[np.newaxis,...] + p_i_z[:,0,:] + beta[:,0,:]\n","    # unnormalised_pi = np.exp(unnormalised_pi).mean(axis=0)\n","    # normalised_pi = unnormalised_pi / unnormalised_pi.sum()    \n","\n","    # for t in range(T-1):\n","        #     unnormalised_A = A[np.newaxis,...] + np.expand_dims(p_i_z[:,t,:], 1) + np.expand_dims(beta[:,t,:], 1)\n","        #     unnormalised_A = np.exp(unnormalised_A).mean(axis=0)\n","        #     normalised_A = unnormalised_A / unnormalised_A.sum(axis=1)[np.newaxis,...]\n","        #     if t<2: print(normalised_A)\n","\n","    return latent_states"]},{"cell_type":"code","execution_count":17,"metadata":{"id":"2W-vQOdbDvT2"},"outputs":[],"source":["def calculate_log_likelihood(pi, A, p_n_ab, p_i_theta, p_z_i, p_zz_i):\n","    # intial state \n","    init = np.sum(np.multiply(np.exp(p_z_i[:,0]), pi[np.newaxis,...]))\n","\n","    # transitional \n","    trans = np.sum(np.multiply(np.exp(p_zz_i), A[np.newaxis, np.newaxis,...]))\n","\n","    # # of items \n","    nbd = np.sum(np.multiply(np.exp(p_z_i), p_n_ab))\n","\n","    # specific item \n","    multi = np.sum(np.multiply(np.exp(p_z_i), p_i_theta))\n","    \n","    return init + trans + nbd + multi"]},{"cell_type":"code","execution_count":40,"metadata":{"id":"-fNCbBYWJEJE"},"outputs":[],"source":["def gibbs_sampling(users_ds, users_Nt, pi_alpha, A_alpha, theta_alpha, n_iterations, burn_in=50):\n","    max_likelihood = None\n","    U, T = users_ds.shape[:-1]\n","    K = len(pi_alpha)\n","\n","    pi, A, theta, a, b = initialise_params_and_probs(pi_alpha, A_alpha, theta_alpha)\n","    latent_states = initialise_latent_states(pi, A, U, T)\n","\n","    # store repeated calculations in NBD and multinomial log prob for significant speed up\n","    multi_first_part = (gammaln(users_Nt + 1) - gammaln(users_ds + 1).sum(axis=-1))[..., np.newaxis]\n","    nbd_first_part = gammaln(users_Nt[..., np.newaxis] + a[np.newaxis, np.newaxis, ...]) \\\n","            - gammaln(a)[np.newaxis, np.newaxis, ...] - gammaln(users_Nt+1)[..., np.newaxis] \n","\n","    pi_bar = np.zeros(pi.shape)\n","    A_bar = np.zeros(A.shape)\n","    theta_bar = np.zeros(theta.shape)\n","    b_bar = np.zeros(b.shape)\n","\n","    # GIBBS SAMPLING\n","    start_time = datetime.now() # for keeping track of running time\n","    for iteration in range(n_iterations + burn_in):\n","\n","        # UPDATE PARAMETERS AND PROBABILITIES\n","        b = update_b(latent_states, users_Nt, K, a[0])\n","        theta = update_theta(latent_states, users_ds, K)\n","        A = update_A(latent_states, K)\n","        pi = update_pi(latent_states, K)\n","\n","        # UPDATE LATENT STATES    \n","        p_i_z, p_n_ab, p_i_theta = calculate_emission_prob(users_ds, users_Nt, nbd_first_part, multi_first_part, theta, a, b)\n","        p_z_i, p_zz_i = calculate_posterior(A, pi, p_i_z, U, T, K)\n","        latent_states = update_all_states(latent_states, p_z_i, p_zz_i)\n","\n","        # CALCULATE EXPECTED LOG LIKELIHOOD\n","        likelihood = calculate_log_likelihood(pi, A, p_n_ab, p_i_theta, p_z_i, p_zz_i)\n","        if max_likelihood is None: \n","            max_likelihood = likelihood\n","        else:\n","            if likelihood > max_likelihood: \n","                max_likelihood = likelihood\n","                pi_max = pi\n","                A_max = A\n","                theta_max = theta\n","                a_max = a\n","                b_max = b\n","\n","        if np.isnan(likelihood):\n","            print('Numerical issues in calculation of log likelihood')\n","            break\n","        print('Iteration', iteration+1,': log likelihood =', likelihood)\n","\n","        if iteration+1 > burn_in:\n","            pi_bar += pi\n","            A_bar += A\n","            theta_bar += theta\n","            b_bar += b\n","\n","    pi_bar /= n_iterations\n","    A_bar /= n_iterations\n","    theta_bar /= n_iterations\n","    b_bar /= n_iterations\n","\n","    run_time = datetime.now() - start_time    \n","    print('Execution time for Gibbs Sampling iterations:', run_time)\n","\n","    return (pi_bar, A_bar, theta_bar, a, b_bar), (pi_max, A_max, theta_max, a_max, b_max)"]},{"cell_type":"code","execution_count":41,"metadata":{"id":"Jhj4RN6nDvT4","outputId":"053bf80e-fede-4d2b-8a55-802d08e1a143"},"outputs":[{"name":"stdout","output_type":"stream","text":["Iteration 1 : log likelihood = -15051166.828040067\n","Iteration 2 : log likelihood = -14956334.337766811\n","Iteration 3 : log likelihood = -14906389.13698416\n","Iteration 4 : log likelihood = -14669163.660391852\n","Iteration 5 : log likelihood = -14515572.75436686\n","Iteration 6 : log likelihood = -14577715.21436321\n","Iteration 7 : log likelihood = -14538491.953784833\n","Iteration 8 : log likelihood = -14581463.645206636\n","Iteration 9 : log likelihood = -14562801.856209667\n","Iteration 10 : log likelihood = -14606311.536621317\n","Iteration 11 : log likelihood = -14583407.89185961\n","Iteration 12 : log likelihood = -14586801.999885539\n","Iteration 13 : log likelihood = -14641624.02922978\n","Iteration 14 : log likelihood = -14645215.639165685\n","Iteration 15 : log likelihood = -14597467.162784915\n","Execution time for Gibbs Sampling iterations: 0:03:10.468270\n"]}],"source":["n_iterations = 10\n","K = 5\n","\n","prior_const = 0.9*K                                       # affects the parameters of the Dirichlet priors\n","pi_alpha = prior_const/K * np.ones((K))                 # alpha hyperparams for pi\n","A_alpha = prior_const/K * np.ones((K,K))                # alpha hyperparams for A\n","theta_alpha = prior_const/K * np.ones((K,N))            # alpha hyperparams for theta\n","\n","expected_params, max_params = gibbs_sampling(users_ds, users_Nt, pi_alpha, A_alpha, theta_alpha, n_iterations, burn_in=5)\n","pi_bar, A_bar, theta_bar, a_bar, b_bar = expected_params\n","pi_max, A_max, theta_max, a_max, b_max = max_params"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"eRG7f9zyDvT5"},"outputs":[],"source":["# save parameters\n","\n","np.save(path + 'pi_bar_K_' + str(K), pi_bar)\n","np.save(path + 'A_bar_K_' + str(K), A_bar)\n","np.save(path + 'a_bar_K_' + str(K), a_bar)\n","np.save(path + 'b_bar_K_' + str(K), b_bar)\n","\n","np.save(path + 'pi_max_K_' + str(K), pi_max)\n","np.save(path + 'A_max_K_' + str(K), A_max)\n","np.save(path + 'a_max_K_' + str(K), a_max)\n","np.save(path + 'b_max_K_' + str(K), b_max)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dlkcaKmiDvT4"},"outputs":[],"source":["%lprun -f gibbs_sampling gibbs_sampling(users_ds, users_Nt, pi_alpha, A_alpha, theta_alpha, n_iterations, 1)"]},{"cell_type":"code","execution_count":24,"metadata":{},"outputs":[],"source":["# Use either the expected parameter values or the max likelihood parameteres\n","\n","# pi, A, theta, a, b = pi_bar, A_bar, theta_bar, a_bar, b_bar\n","\n","pi, A, theta, a, b = pi_max, A_max, theta_max, a_max, b_max"]},{"cell_type":"code","execution_count":25,"metadata":{"id":"PNGezp-ADvT5","outputId":"b9519b9f-b5ab-4b3f-dd07-f34aa86bde57"},"outputs":[{"name":"stdout","output_type":"stream","text":["-14451414.156572726\n"]}],"source":["multi_first_part = (gammaln(users_Nt + 1) - gammaln(users_ds + 1).sum(axis=-1))[..., np.newaxis]\n","nbd_first_part = gammaln(users_Nt[..., np.newaxis] + a[np.newaxis, np.newaxis, ...]) \\\n","        - gammaln(a)[np.newaxis, np.newaxis, ...] - gammaln(users_Nt+1)[..., np.newaxis] \n","p_i_z, p_n_ab, p_i_theta = calculate_emission_prob(users_ds, users_Nt, nbd_first_part, multi_first_part, theta, a, b)\n","p_z_i, p_zz_i = calculate_posterior(A, pi, p_i_z, U, T, K)\n","likelihood = calculate_log_likelihood(pi, A, p_n_ab, p_i_theta, p_z_i, p_zz_i)\n","print(likelihood)"]},{"cell_type":"code","execution_count":26,"metadata":{"id":"5J2a9-87DvT5","outputId":"47d23423-a55b-4bf7-abd3-6e1054da6bd5"},"outputs":[{"name":"stdout","output_type":"stream","text":["   Log Likelihood  Iterations  # movies rated in test period  \\\n","0   -1.445141e+07          10                          60499   \n","\n","   Total +ve for top 5  Precision of top 5  Recall of top 5  \\\n","0                  161            0.026568         0.002661   \n","\n","   Total +ve for top 10  Precision of top 10  Recall of top 10  \n","0                   306             0.025248          0.005058  \n"]}],"source":["# number of items to recommend\n","num_items = 5000\n","\n","# log prob of user each latent class in next period assuming user in Z(t) with log p(Z(t)|I(1:T))\n","# result is multiplying transitional prob to prob of user in each latent class at time t\n","p_z = logdotexp(p_z_i[:,-1], A)\n","\n","# calculate probability that item i is not read in the next time period\n","p_noti_z = np.power(1 + b[...,np.newaxis] * np.exp(theta), -a[...,np.newaxis])\n","\n","# calculate rank score of the items likely to appear in next time period\n","rank_score = -np.exp(p_z) @ p_noti_z\n","\n","# generate indices of top num_items to recommend which will be unsorted\n","rec_list = np.argpartition(rank_score, -num_items, axis=-1)[:,-num_items:]\n","\n","# sort indices by rank score\n","rec_list_score = np.array([row[rec_list[i,:]] for i, row in enumerate(rank_score)]) # get the scores of items in rec_list\n","sorted_rec_list = np.array([row[np.flip(np.argsort(rec_list_score[i]))] for i, row in enumerate(rec_list)]) # sort the rec_list based on the score\n","\n","# check if item in user history\n","user_history = np.array([row[:,sorted_rec_list[i]] for i, row in enumerate(users_ds)]) # get all values in user_ds corresponding to the item in rec_list for each user in each time period\n","user_history = np.sum(user_history, axis=1) # get boolean array indicating whether each item in sorted_rec_list is in user history (assumes user only has each item at most once)\n","if user_history.max() > 1: print('There are repeated ratings of a movie by at least one user')\n","# print(user_history.shape)\n","\n","# filter sorted_rec_list for items not in user history\n","filtered_rec_list = [row[np.logical_not(user_history[i])] for i, row in enumerate(sorted_rec_list)] # each user's list will not have the same amount of items as it depends on user history\n","\n","# get multi-hot encoding of top N recommended movies for the next period\n","mlb = MultiLabelBinarizer(classes=range(N), sparse_output=False) # prediction done on based on one hot encoding indexing i.e. starting index is 0\n","top_5_list = [mlb.fit_transform([user[:5]]) for user in filtered_rec_list] # convert top 5 list to one hot encoding\n","top_10_list = [mlb.fit_transform([user[:10]]) for user in filtered_rec_list] \n","\n","# test how many of top N recommended movies appear in user's rated list of movies in the test period\n","positive_top_5 = [np.multiply(test_ds[i], rec_user) for i, rec_user in enumerate(top_5_list)] # get (#users,#items) boolean vectors indicating whether recommended movie was rating in test period\n","users_result_top_5 = [row.sum() for row in positive_top_5] # get list of positive matches per user\n","all_result_top_5 = np.sum(users_result_top_5) # total number of positive matches across all users\n","\n","positive_top_10 = [np.multiply(test_ds[i], rec_user) for i, rec_user in enumerate(top_10_list)] \n","users_result_top_10 = [row.sum() for row in positive_top_10] # get list of positive matches per user\n","all_result_top_10 = np.sum(users_result_top_10) # total number of positive matches across all users\n","\n","# total number of movies rated in test period\n","test_num_movies_rated = np.sum(test_ds).sum()\n","\n","# output results to excel via pandas df\n","dict_result = {'Log Likelihood':likelihood, 'Iterations':n_iterations,            \n","            '# movies rated in test period': test_num_movies_rated, \n","            'Total +ve for top 5':all_result_top_5, \n","            'Precision of top 5':all_result_top_5/(5*U),\n","            'Recall of top 5':all_result_top_5/test_num_movies_rated,\n","            'Total +ve for top 10':all_result_top_10,\n","            'Precision of top 10':all_result_top_10/(10*U),\n","            'Recall of top 10':all_result_top_10/test_num_movies_rated\n","            }\n","df_result = pd.DataFrame(data=dict_result, index=[0])\n","print(df_result)\n","df_result.to_csv(path + 'result.csv', index=False)"]},{"cell_type":"markdown","metadata":{"id":"BX6LNJq3-aMe"},"source":["# Testing of Sampling Speed"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":8324,"status":"ok","timestamp":1649384424919,"user":{"displayName":"Chung I Lu","userId":"13493253019536068280"},"user_tz":-480},"id":"dROuEu967z1V","outputId":"1710abc5-9c0d-450f-c057-26cd07f7a888"},"outputs":[{"name":"stdout","output_type":"stream","text":["1 loop, best of 5: 1.28 s per loop\n"]}],"source":["%%timeit\n","\n","theta = np.random.dirichlet(alpha=theta_alpha[0], size=1).squeeze()\n","test = np.random.multinomial(n=50, pvals=theta, size=U).squeeze()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":8101,"status":"ok","timestamp":1649384434374,"user":{"displayName":"Chung I Lu","userId":"13493253019536068280"},"user_tz":-480},"id":"nWENKHM89n-q","outputId":"a59e224f-6185-4338-ab6d-ea49f08949dc"},"outputs":[{"name":"stdout","output_type":"stream","text":["1 loop, best of 5: 1.27 s per loop\n"]}],"source":["%%timeit\n","\n","theta = stats.dirichlet.rvs(alpha=theta_alpha[0], size=1).squeeze()\n","test = stats.multinomial.rvs(n=50, p=theta, size=U).squeeze()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":7502,"status":"ok","timestamp":1649487189957,"user":{"displayName":"Chung I Lu","userId":"13493253019536068280"},"user_tz":-480},"id":"Wx5KpdTvF3y6","outputId":"8542e991-3926-4771-d70c-a8f7aad30593"},"outputs":[{"name":"stdout","output_type":"stream","text":["1 loop, best of 5: 1.12 s per loop\n"]}],"source":["%%timeit\n","\n","theta = rng.dirichlet(alpha=theta_alpha[0], size=1).squeeze()\n","test = rng.multinomial(50, theta, size=U,).squeeze()"]}],"metadata":{"colab":{"collapsed_sections":[],"machine_shape":"hm","name":"Gibbs Sampling for HMM.ipynb","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.7"}},"nbformat":4,"nbformat_minor":0}
